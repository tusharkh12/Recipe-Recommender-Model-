{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:32:59.619315Z",
     "start_time": "2024-02-03T07:32:58.497901Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Note: The following do not work with Python 3.12\n",
    "import shap\n",
    "# from ydata_profiling import ProfileReport\n",
    "import sweetviz as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reproducibility:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b211a10ac5cb06b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "seed = 2024\n",
    "\n",
    "# pandas, statsmodels, matplotlib and y_data_profiling rely on numpy's random generator, and thus, we need to set the seed in numpy\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:33:11.936583Z",
     "start_time": "2024-02-03T07:33:11.920010Z"
    }
   },
   "id": "4f3562cc1424ebc8",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Understanding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62c572c4be038d12"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "diet = pd.read_csv('diet.csv', low_memory=False)\n",
    "diet['Diet'] = diet['Diet'].astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:36:55.807774Z",
     "start_time": "2024-02-03T07:36:55.646392Z"
    }
   },
   "id": "8a203101be6504ee",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Box Plot of Ages by Diet: Explore relationships between numerical variables (Age) using a pair plot.\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='Diet', y='Age', data=diet, palette='pastel')\n",
    "plt.title('Box Plot of Ages by Diet')\n",
    "plt.xlabel('Diet')\n",
    "plt.ylabel('Age')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3115e15b8065ed20"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "requests = pd.read_csv('requests.csv', low_memory=False)\n",
    "requests['HighProtein'] = requests['HighProtein'].astype('category')\n",
    "requests['LowSugar'] = requests['LowSugar'].astype('category')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:36:59.169579Z",
     "start_time": "2024-02-03T07:36:59.034653Z"
    }
   },
   "id": "5bb2dbf6d2c0fc3f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Correlation Heatmap: Visualize the correlation between numerical variables.\n",
    "# Exclude non-numeric columns\n",
    "numeric_columns = requests[['Time', 'HighCalories', 'LowFat', 'HighFiber']]\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Correlation Heatmap for Request Variables')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d89e49915d7fe069"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.csv', low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:18.290044Z",
     "start_time": "2024-02-03T07:37:18.220303Z"
    }
   },
   "id": "cfd0b14270e60d2",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Scatter Plot of Rating vs Like:Investigate the relationship between ratings and likes using a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reviews['Rating'], reviews['Like'], alpha=0.5, color='green')\n",
    "plt.title('Scatter Plot of Rating vs Like')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Like')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14c32fccb919d63e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "recipes = pd.read_csv('recipes.csv', low_memory=False)\n",
    "recipes.rename(columns={\n",
    "    'Name': 'RecipeName'\n",
    "}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:02.470279Z",
     "start_time": "2024-02-03T07:37:01.903613Z"
    }
   },
   "id": "dd22b260ae91da1d",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Histogram for Calories:Explore the distribution of calories in recipes using a histogram.\n",
    "custom_bins = np.arange(0, 5100, 200)  # Adjust the range and interval as needed\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(recipes['Calories'], bins=custom_bins, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Calories in Recipes')\n",
    "plt.xlabel('Calories')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Set x-axis labels\n",
    "plt.xticks(custom_bins)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1684c39f77419a7d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Bar Plot for Recipe Categories: Visualize the distribution of recipes across different categories using a bar plot.\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x='RecipeCategory', y='CookTime', data=recipes, palette='Set2')\n",
    "plt.title('Box Plot of Cook Time by Recipe Category')\n",
    "plt.xlabel('Recipe Category')\n",
    "plt.ylabel('Cook Time')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e934334a4a507210"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_request_review = pd.merge(reviews,requests,on=['AuthorId','RecipeId'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:21.938206Z",
     "start_time": "2024-02-03T07:37:21.845231Z"
    }
   },
   "id": "3a2e6cf72d8652ba",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#eatmap for Correlation: Visualize the correlation between numerical variables.\n",
    "correlation_matrix = merged_request_review[['Rating', 'Like', 'Time', 'HighCalories', 'LowFat', 'HighFiber']].corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Correlation Heatmap for Merged Data')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12684a8daf250102"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Joining using common attributes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9f899608c6f8d47"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_recipes_req_review= pd.merge(merged_request_review,recipes,on=['RecipeId'],how='right')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:25.309983Z",
     "start_time": "2024-02-03T07:37:25.234811Z"
    }
   },
   "id": "fb968b7390002997",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "author_ID = 'AuthorId'\n",
    "merged_diet_all = pd.merge(diet, merged_recipes_req_review, on=author_ID)\n",
    "# merged_request_recipes = pd.merge(requests, recipes, on='RecipeId', how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:36.814806Z",
     "start_time": "2024-02-03T07:37:36.611290Z"
    }
   },
   "id": "49daee29ad9a6b88",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Impute the missing values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67ef3cd2b0bb2848"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 1 value missing in diet column. Filled with most occuring value.\n",
    "merged_diet_all['Diet'] = merged_diet_all['Diet'].fillna('Vegetarian')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:38.864023Z",
     "start_time": "2024-02-03T07:37:38.859648Z"
    }
   },
   "id": "d7bc65da7602be7d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n"
     ]
    }
   ],
   "source": [
    "#impute the values for all dietary preferences for all ages with the most frequent RecipeId for that age in that category\n",
    "helper_df = merged_diet_all.groupby(['Age', 'Diet'])['RecipeId'].agg(lambda x: x.mode()[0]).reset_index()\n",
    "helper_df.columns = ['Age', 'Diet', 'Most Common Recipe']\n",
    "def impute_recipe(row):\n",
    "    if pd.isnull(row['RecipeId']):\n",
    "        return helper_df[(helper_df['Age'] == row['Age']) & (helper_df['Diet'] == row['Diet'])]['Most Common Recipe'].values[0]\n",
    "    else:\n",
    "        return row['RecipeId']\n",
    "merged_diet_all['RecipeId'] = merged_diet_all.apply(impute_recipe, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:42.335080Z",
     "start_time": "2024-02-03T07:37:41.323785Z"
    }
   },
   "id": "56bb590b3ee5cfc8",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fill the rest of the missing values in the merged_diet_all by mapping them from requests.csv with RecipeId as key \n",
    "# Create mapping DataFrames from `requests`\n",
    "map_time = requests.set_index('RecipeId')['Time'].to_dict()\n",
    "map_calories = requests.set_index('RecipeId')['HighCalories'].to_dict()\n",
    "map_protein = requests.set_index('RecipeId')['HighProtein'].to_dict()\n",
    "map_fat = requests.set_index('RecipeId')['LowFat'].to_dict()\n",
    "map_sugar = requests.set_index('RecipeId')['LowSugar'].to_dict()\n",
    "map_fiber = requests.set_index('RecipeId')['HighFiber'].to_dict()\n",
    "\n",
    "# Apply mapping to `merged_diet_all`\n",
    "merged_diet_all['Time'] = merged_diet_all['RecipeId'].map(map_time)\n",
    "merged_diet_all['HighCalories'] = merged_diet_all['RecipeId'].map(map_calories)\n",
    "merged_diet_all['HighProtein'] = merged_diet_all['RecipeId'].map(map_protein)\n",
    "merged_diet_all['LowFat'] = merged_diet_all['RecipeId'].map(map_fat)\n",
    "merged_diet_all['LowSugar'] = merged_diet_all['RecipeId'].map(map_sugar)\n",
    "merged_diet_all['HighFiber'] = merged_diet_all['RecipeId'].map(map_fiber)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:46.099127Z",
     "start_time": "2024-02-03T07:37:45.677676Z"
    }
   },
   "id": "d6ff0b6af8d3e1b2",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "map_name = recipes.set_index('RecipeId')['RecipeName'].to_dict()\n",
    "map_cook_time = recipes.set_index('RecipeId')['CookTime'].to_dict()\n",
    "map_prep_time = recipes.set_index('RecipeId')['PrepTime'].to_dict()\n",
    "map_category = recipes.set_index('RecipeId')['RecipeCategory'].to_dict()\n",
    "map_quantities = recipes.set_index('RecipeId')['RecipeIngredientQuantities'].to_dict()\n",
    "map_parts = recipes.set_index('RecipeId')['RecipeIngredientParts'].to_dict()\n",
    "map_calories = recipes.set_index('RecipeId')['Calories'].to_dict()\n",
    "map_fat_content = recipes.set_index('RecipeId')['FatContent'].to_dict()\n",
    "map_saturated_content = recipes.set_index('RecipeId')['SaturatedFatContent'].to_dict()\n",
    "map_cholesterol = recipes.set_index('RecipeId')['CholesterolContent'].to_dict()\n",
    "map_sodium = recipes.set_index('RecipeId')['SodiumContent'].to_dict()\n",
    "map_carbohydrate = recipes.set_index('RecipeId')['CarbohydrateContent'].to_dict()\n",
    "map_fiber = recipes.set_index('RecipeId')['FiberContent'].to_dict()\n",
    "map_sugar = recipes.set_index('RecipeId')['SugarContent'].to_dict()\n",
    "map_protein = recipes.set_index('RecipeId')['ProteinContent'].to_dict()\n",
    "map_servings = recipes.set_index('RecipeId')['RecipeServings'].to_dict()\n",
    "map_yield = recipes.set_index('RecipeId')['RecipeYield'].to_dict()\n",
    "\n",
    "\n",
    "# Apply mapping to `merged_diet_all`\n",
    "merged_diet_all['RecipeName'] = merged_diet_all['RecipeId'].map(map_name)\n",
    "merged_diet_all['CookTime'] = merged_diet_all['RecipeId'].map(map_cook_time)\n",
    "merged_diet_all['PrepTime'] = merged_diet_all['RecipeId'].map(map_prep_time)\n",
    "merged_diet_all['RecipeCategory'] = merged_diet_all['RecipeId'].map(map_category)\n",
    "merged_diet_all['RecipeIngredientQuantities'] = merged_diet_all['RecipeId'].map(map_quantities)\n",
    "merged_diet_all['RecipeIngredientParts'] = merged_diet_all['RecipeId'].map(map_parts)\n",
    "merged_diet_all['Calories'] = merged_diet_all['RecipeId'].map(map_calories)\n",
    "merged_diet_all['FatContent'] = merged_diet_all['RecipeId'].map(map_fat)\n",
    "merged_diet_all['SaturatedFatContent'] = merged_diet_all['RecipeId'].map(map_saturated_content)\n",
    "merged_diet_all['CholesterolContent'] = merged_diet_all['RecipeId'].map(map_cholesterol)\n",
    "merged_diet_all['SodiumContent'] = merged_diet_all['RecipeId'].map(map_sodium)\n",
    "merged_diet_all['CarbohydrateContent'] = merged_diet_all['RecipeId'].map(map_carbohydrate)\n",
    "merged_diet_all['FiberContent'] = merged_diet_all['RecipeId'].map(map_fiber)\n",
    "merged_diet_all['SugarContent'] = merged_diet_all['RecipeId'].map(map_sugar)\n",
    "merged_diet_all['ProteinContent'] = merged_diet_all['RecipeId'].map(map_protein)\n",
    "merged_diet_all['RecipeServings'] = merged_diet_all['RecipeId'].map(map_servings)\n",
    "merged_diet_all['RecipeYield'] = merged_diet_all['RecipeId'].map(map_yield)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:47.578900Z",
     "start_time": "2024-02-03T07:37:46.098205Z"
    }
   },
   "id": "97229f12930cb918",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_diet_all['Rating'] = merged_diet_all['Rating'].fillna(0)\n",
    "merged_diet_all['TestSetId'] = merged_diet_all['TestSetId']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:47.931412Z",
     "start_time": "2024-02-03T07:37:47.922962Z"
    }
   },
   "id": "2c72759d12f3b5ce",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "Balance the proportion of True and False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d19907777292ad2e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like\n",
      "True     0.5\n",
      "False    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = merged_diet_all\n",
    "# Calculate the proportion of true and false values in the 'Like' column\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Calculate the proportion of true and false values in the 'Like' column\n",
    "proportion_true = df['Like'].mean()\n",
    "proportion_false = 1 - proportion_true\n",
    "\n",
    "# Identify the indices of false values\n",
    "false_indices = df[df['Like'] == False].index\n",
    "\n",
    "# Randomly sample false indices to achieve a balanced proportion\n",
    "num_false_to_sample = int(df['Like'].value_counts()[True] / proportion_true) - df['Like'].value_counts()[False]\n",
    "indices_to_sample = np.random.choice(false_indices, size=num_false_to_sample, replace=False)\n",
    "\n",
    "# Create a new DataFrame with sampled false values\n",
    "df_balanced = pd.concat([df[df['Like'] == True], df.loc[indices_to_sample]])\n",
    "\n",
    "# Shuffle the new DataFrame to randomize the order\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Print the proportion of true and false values in the balanced DataFrame\n",
    "print(df_balanced['Like'].value_counts(normalize=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:37:51.441814Z",
     "start_time": "2024-02-03T07:37:51.345457Z"
    }
   },
   "id": "2a503f2efaf3093a",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a Gradient Boosting Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7effdf9c3873a9a7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy on Training Set: 0.7730\n",
      "Balanced Accuracy on Testing Set: 0.7590\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "# 72.7 WITH 2000 ADD\n",
    "# Load your data\n",
    "df = df_balanced\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "features = ['Diet', 'Age', 'Rating', 'Time', 'HighCalories','HighProtein','LowFat','LowSugar','HighFiber', 'CookTime', 'PrepTime','RecipeCategory', 'RecipeCategory', 'Calories', 'FatContent','SaturatedFatContent', 'CholesterolContent', 'SodiumContent',  'CarbohydrateContent','FiberContent', 'SugarContent', 'ProteinContent']\n",
    "\n",
    "\n",
    "target = 'Like'\n",
    "\n",
    "# Drop rows with NaN values in the target column ('Like') and create a copy for training set\n",
    "train_data = df.dropna(subset=[target]).copy()\n",
    "\n",
    "# Convert 'Like' column to boolean (if not already)\n",
    "train_data['Like'] = train_data['Like'].astype(bool)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = train_data[features]\n",
    "y = train_data['Like']\n",
    "\n",
    "# Handle categorical variables using one-hot encoding\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "model = GradientBoostingClassifier(\n",
    "    n_estimators=100,     # You can adjust this\n",
    "    random_state=42,\n",
    "    learning_rate=0.1,    # You can adjust this\n",
    "    max_depth=3,          # You can adjust this\n",
    "    min_samples_split=2,  # You can adjust this\n",
    "    min_samples_leaf=1,   # You can adjust this\n",
    "    subsample=1.0         # You can adjust this\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "# Compute balanced accuracy for the training set\n",
    "balanced_accuracy_train = balanced_accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Balanced Accuracy on Training Set: {balanced_accuracy_train:.4f}\")\n",
    "\n",
    "# Predict the 'Like' column for the testing set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Compute balanced accuracy for the testing set\n",
    "balanced_accuracy_test = balanced_accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Balanced Accuracy on Testing Set: {balanced_accuracy_test:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T07:47:08.800068Z",
     "start_time": "2024-02-03T07:47:00.667918Z"
    }
   },
   "id": "9cf788ae6509e47e",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "550/550 [==============================] - 1s 819us/step - loss: 0.6583 - accuracy: 0.6066 - val_loss: 0.6072 - val_accuracy: 0.6811\n",
      "Epoch 2/20\n",
      "550/550 [==============================] - 0s 698us/step - loss: 0.5900 - accuracy: 0.6909 - val_loss: 0.5880 - val_accuracy: 0.6904\n",
      "Epoch 3/20\n",
      "550/550 [==============================] - 0s 657us/step - loss: 0.5751 - accuracy: 0.6991 - val_loss: 0.5775 - val_accuracy: 0.6940\n",
      "Epoch 4/20\n",
      "550/550 [==============================] - 0s 675us/step - loss: 0.5652 - accuracy: 0.7076 - val_loss: 0.5723 - val_accuracy: 0.7013\n",
      "Epoch 5/20\n",
      "550/550 [==============================] - 0s 665us/step - loss: 0.5577 - accuracy: 0.7107 - val_loss: 0.5690 - val_accuracy: 0.7008\n",
      "Epoch 6/20\n",
      "550/550 [==============================] - 0s 660us/step - loss: 0.5518 - accuracy: 0.7155 - val_loss: 0.5637 - val_accuracy: 0.7037\n",
      "Epoch 7/20\n",
      "550/550 [==============================] - 0s 662us/step - loss: 0.5492 - accuracy: 0.7172 - val_loss: 0.5626 - val_accuracy: 0.7074\n",
      "Epoch 8/20\n",
      "550/550 [==============================] - 0s 672us/step - loss: 0.5450 - accuracy: 0.7203 - val_loss: 0.5600 - val_accuracy: 0.7074\n",
      "Epoch 9/20\n",
      "550/550 [==============================] - 0s 665us/step - loss: 0.5423 - accuracy: 0.7215 - val_loss: 0.5573 - val_accuracy: 0.7112\n",
      "Epoch 10/20\n",
      "550/550 [==============================] - 1s 1ms/step - loss: 0.5394 - accuracy: 0.7255 - val_loss: 0.5529 - val_accuracy: 0.7105\n",
      "Epoch 11/20\n",
      "550/550 [==============================] - 0s 659us/step - loss: 0.5378 - accuracy: 0.7260 - val_loss: 0.5530 - val_accuracy: 0.7163\n",
      "Epoch 12/20\n",
      "550/550 [==============================] - 0s 654us/step - loss: 0.5363 - accuracy: 0.7258 - val_loss: 0.5508 - val_accuracy: 0.7124\n",
      "Epoch 13/20\n",
      "550/550 [==============================] - 0s 656us/step - loss: 0.5344 - accuracy: 0.7254 - val_loss: 0.5496 - val_accuracy: 0.7178\n",
      "Epoch 14/20\n",
      "550/550 [==============================] - 0s 649us/step - loss: 0.5335 - accuracy: 0.7274 - val_loss: 0.5495 - val_accuracy: 0.7195\n",
      "Epoch 15/20\n",
      "550/550 [==============================] - 0s 652us/step - loss: 0.5321 - accuracy: 0.7295 - val_loss: 0.5473 - val_accuracy: 0.7188\n",
      "Epoch 16/20\n",
      "550/550 [==============================] - 0s 653us/step - loss: 0.5309 - accuracy: 0.7300 - val_loss: 0.5459 - val_accuracy: 0.7207\n",
      "Epoch 17/20\n",
      "550/550 [==============================] - 0s 656us/step - loss: 0.5305 - accuracy: 0.7309 - val_loss: 0.5489 - val_accuracy: 0.7180\n",
      "Epoch 18/20\n",
      "550/550 [==============================] - 0s 653us/step - loss: 0.5294 - accuracy: 0.7331 - val_loss: 0.5459 - val_accuracy: 0.7207\n",
      "Epoch 19/20\n",
      "550/550 [==============================] - 0s 653us/step - loss: 0.5285 - accuracy: 0.7341 - val_loss: 0.5443 - val_accuracy: 0.7226\n",
      "Epoch 20/20\n",
      "550/550 [==============================] - 0s 672us/step - loss: 0.5284 - accuracy: 0.7340 - val_loss: 0.5420 - val_accuracy: 0.7212\n",
      "161/161 [==============================] - 0s 396us/step\n",
      "Balanced Accuracy on Testing Set: 0.7055\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load your data\n",
    "df = df_balanced\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "features = ['Diet', 'Age', 'Rating', 'Time', 'HighCalories', 'HighProtein', 'LowFat', 'LowSugar', 'HighFiber',\n",
    "            'CookTime', 'PrepTime', 'RecipeCategory', 'RecipeCategory', 'Calories', 'FatContent',\n",
    "            'SaturatedFatContent', 'CholesterolContent', 'SodiumContent', 'CarbohydrateContent', 'FiberContent',\n",
    "            'SugarContent', 'ProteinContent']\n",
    "\n",
    "target = 'Like'\n",
    "\n",
    "# Drop rows with NaN values in the target column ('Like') and create a copy for the training set\n",
    "train_data = df.dropna(subset=[target]).copy()\n",
    "\n",
    "# Convert 'Like' column to boolean (if not already)\n",
    "train_data['Like'] = train_data['Like'].astype(bool)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = train_data[features]\n",
    "y = train_data['Like']\n",
    "\n",
    "# Handle categorical variables using one-hot encoding\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Neural Network model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 10 neurons and 'relu' activation function\n",
    "model.add(Dense(units=10, activation='relu', input_dim=X_train_scaled.shape[1]))\n",
    "\n",
    "# Add the second hidden layer with 5 neurons and 'relu' activation function\n",
    "model.add(Dense(units=5, activation='relu'))\n",
    "\n",
    "# Output layer with 1 neuron and 'sigmoid' activation function for binary classification\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on the training set\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=30, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
    "\n",
    "# Compute balanced accuracy for the testing set\n",
    "balanced_accuracy_test = balanced_accuracy_score(y_test, y_test_pred_binary)\n",
    "print(f\"Balanced Accuracy on Testing Set: {balanced_accuracy_test:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T08:19:59.488888Z",
     "start_time": "2024-02-03T08:19:51.252647Z"
    }
   },
   "id": "56413903366c085d",
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training a Random Forest Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92dcda8ee998b6fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from itertools import combinations\n",
    "\n",
    "# Load your data\n",
    "df = df_balanced\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "features = ['Diet','Age', 'Rating', 'Time', 'CookTime', 'FiberContent', 'ProteinContent', 'SugarContent',  'SodiumContent', 'CholesterolContent', 'SaturatedFatContent', 'FatContent', 'Calories', 'HighFiber', 'HighCalories', 'RecipeCategory', 'HighProtein']\n",
    "target = 'Like'\n",
    "\n",
    "# Drop rows with NaN values in the target column ('Like') and create a copy for training set\n",
    "train_data = df.dropna(subset=[target]).copy()\n",
    "\n",
    "# Convert 'Like' column to boolean (if not already)\n",
    "train_data['Like'] = train_data['Like'].astype(bool)\n",
    "\n",
    "# Get all possible combinations of features\n",
    "\n",
    "\n",
    "best_combination = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Iterate over all feature combinations\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = train_data[features]\n",
    "y = train_data['Like']\n",
    "\n",
    "# Handle categorical variables using one-hot encodin\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest model with hyperparameter tuning\n",
    "model_final_rfc = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=47,\n",
    "    max_depth=10,\n",
    "    min_samples_split=6,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt'\n",
    ")\n",
    "\n",
    "# Fit the model on the training set\n",
    "model_final_rfc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_test_pred = model_final_rfc.predict(X_test)\n",
    "balanced_accuracy_train = balanced_accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Balanced Accuracy on Training Set: {balanced_accuracy_train:.4f}\")\n",
    "# Compute balanced accuracy for the testing set\n",
    "balanced_accuracy_test = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the feature combination and its balanced accuracy\n",
    "# print(f\"Balanced Accuracy on Training Set: {balanced_accuracy_test:.4f}\")\n",
    "\n",
    "print(f\"Balanced Accuracy on Testing Set: {balanced_accuracy_test:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92d4216a322fe1d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deploy the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5b7564d1ae0209"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "features = ['Diet','Age', 'Rating', 'Time', 'CookTime', 'FiberContent', 'ProteinContent', 'SugarContent',  'SodiumContent', 'CholesterolContent', 'SaturatedFatContent', 'FatContent', 'Calories', 'HighFiber', 'HighCalories', 'RecipeCategory', 'HighProtein','TestSetId','Like']\n",
    "df = merged_diet_all[features].copy()\n",
    "\n",
    "# Drop and save testsetid\n",
    "# Drop rows with NaN values in the target column ('Like') for training set\n",
    "predict_data = df[df['Like'].isna()]\n",
    "predict_data\n",
    "\n",
    "predict_data = predict_data.sort_values(by='TestSetId', ascending=True)\n",
    "predict_data\n",
    "# 1. One-Hot Encode Categorical Variables in predict_data\n",
    "predict_features = predict_data[features]  # 'features' list from your model training code\n",
    "predict_features_encoded = pd.get_dummies(predict_features)\n",
    "\n",
    "# 2. Align the Columns of predict_data with X_train\n",
    "# Add missing columns in predict_features_encoded with value equal to 0\n",
    "missing_cols = set(X_train.columns) - set(predict_features_encoded.columns)\n",
    "for c in missing_cols:\n",
    "    predict_features_encoded[c] = 0\n",
    "\n",
    "# Ensure the order of columns in predict_features_encoded matches that of X_train\n",
    "predict_features_encoded = predict_features_encoded[X_train.columns]\n",
    "\n",
    "# 3. Make Predictions Using the Trained Model\n",
    "predicted_likes = model_final_rfc.predict(predict_features_encoded)\n",
    "\n",
    "# Combine Predictions with 'TestSetId'\n",
    "results_df = pd.DataFrame({\n",
    "    'id': predict_data['TestSetId'],\n",
    "    'prediction': predicted_likes\n",
    "})\n",
    "results_df['id'] = results_df['id'].astype(int)\n",
    "results_df['prediction'] = results_df['prediction'].replace({True: 1, False: 0})\n",
    "\n",
    "\n",
    "# Display the first few rows of the results\n",
    "print(results_df.head())\n",
    "results_df.to_csv('predictions_skilled_shark.csv',index=False)\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e39e96dc4ff794db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
